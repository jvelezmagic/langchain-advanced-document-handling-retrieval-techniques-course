{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perdido en el medio: El problema con los contextos largos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Independientemente de la arquitectura de tu modelo, existe una degradaci√≥n sustancial del rendimiento cuando incluyes m√°s de 10 documentos recuperados. En resumen: Cuando los modelos deben acceder a informaci√≥n relevante en medio de contextos largos, tienden a ignorar los documentos proporcionados. Ver: https://arxiv.org/abs/2307.03172\n",
    "\n",
    "Para evitar este problema, puedes reordenar los documentos despu√©s de recuperarlos para evitar la degradaci√≥n del rendimiento.\"\n",
    "\n",
    "---\n",
    "[Langchain](https://python.langchain.com/docs/modules/data_connection/document_transformers/post_retrieval/long_context_reorder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Librer√≠as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_transformers import LongContextReorder\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "from src.langchain_docs_loader import LangchainDocsLoader, num_tokens_from_string\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carga de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=350,\n",
    "    chunk_overlap=10,\n",
    "    length_function=num_tokens_from_string,\n",
    ")\n",
    "\n",
    "documents = LangchainDocsLoader().load()\n",
    "documents = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creaci√≥n de retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-embedding-ada-002 in organization org-vwqjdaXGZeEg6mWAVSflJXD9 on tokens per min. Limit: 1000000 / min. Current: 774651 / min. Contact us through our help center at help.openai.com if you continue to have issues..\n"
     ]
    }
   ],
   "source": [
    "retriever = Chroma.from_documents(documents, embedding=OpenAIEmbeddings()).as_retriever(\n",
    "    search_type=\"mmr\",\n",
    "    search_kwargs={\n",
    "        \"k\": 10,\n",
    "        \"fetch_k\": 50,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consulta con el retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='## LLMRails as a Retriever\\u200b\\n\\nLLMRails, as all the other LangChain vectorstores, is most often used as a LangChain Retriever:\\n\\n```python\\nretriever = llm_rails.as_retriever()\\nretriever\\n```\\n\\n```text\\n    LLMRailsRetriever(tags=None, metadata=None, vectorstore=<langchain.vectorstores.llm_rails.LLMRails object at 0x107b9c040>, search_type=\\'similarity\\', search_kwargs={\\'k\\': 5})\\n```\\n\\n```python\\nquery = \"What is your approach to national defense\"\\nretriever.get_relevant_documents(query)[0]\\n```', metadata={'description': 'LLMRails is a API platform for building GenAI applications. It provides an easy-to-use API for document indexing and querying that is managed by LLMRails and is optimized for performance and accuracy.', 'language': 'en', 'source': 'https://python.langchain.com/docs/integrations/vectorstores/llm_rails', 'title': 'LLMRails | ü¶úÔ∏èüîó Langchain'}),\n",
       " Document(page_content=\"QdrantTranslator\\\\n6. WeaviateTranslator\\\\n\\\\nAnd remote retrievers like:\\\\n\\\\n1. RemoteLangChainRetriever'}\", metadata={'description': \"In this tutorial, we are going to use Langchain + Activeloop's Deep Lake with GPT to analyze the code base of the LangChain itself.\", 'language': 'en', 'source': 'https://python.langchain.com/docs/use_cases/question_answering/how_to/code/code-analysis-deeplake', 'title': \"Use LangChain, GPT and Activeloop's Deep Lake to work with code base | ü¶úÔ∏èüîó Langchain\"}),\n",
       " Document(page_content='# Retrieve as you generate with FLARE\\n\\nThis notebook is an implementation of Forward-Looking Active REtrieval augmented generation (FLARE).\\n\\nPlease see the original repo [here](https://github.com/jzbjyb/FLARE/tree/main).\\n\\nThe basic idea is:\\n\\n- Start answering a question\\n- If you start generating tokens the model is uncertain about, look up relevant documents\\n- Use those documents to continue generating\\n- Repeat until finished\\n\\nThere is a lot of cool detail in how the lookup of relevant documents is done.\\nBasically, the tokens that model is uncertain about are highlighted, and then an LLM is called to generate a question that would lead to that answer. For example, if the generated text is `Joe Biden went to Harvard`, and the tokens the model was uncertain about was `Harvard`, then a good generated question would be `where did Joe Biden go to college`. This generated question is then used in a retrieval step to fetch relevant documents.\\n\\nIn order to set up this chain, we will need three things:\\n\\n- An LLM to generate the answer\\n- An LLM to generate hypothetical questions to use in retrieval\\n- A retriever to use to look up answers for\\n\\nThe LLM that we use to generate the answer needs to return logprobs so we can identify uncertain tokens. For that reason, we HIGHLY recommend that you use the OpenAI wrapper (NB: not the ChatOpenAI wrapper, as that does not return logprobs).\\n\\nThe LLM we use to generate hypothetical questions to use in retrieval can be anything. In this notebook we will use ChatOpenAI because it is fast and cheap.', metadata={'description': 'This notebook is an implementation of Forward-Looking Active REtrieval augmented generation (FLARE).', 'language': 'en', 'source': 'https://python.langchain.com/docs/use_cases/question_answering/how_to/flare', 'title': 'Retrieve as you generate with FLARE | ü¶úÔ∏èüîó Langchain'}),\n",
       " Document(page_content='# Self-querying\\n\\nA self-querying retriever is one that, as the name suggests, has the ability to query itself. Specifically, given any natural language query, the retriever uses a query-constructing LLM chain to write a structured query and then applies that structured query to its underlying VectorStore. This allows the retriever to not only use the user-input query for semantic similarity comparison with the contents of stored documents but to also extract filters from the user query on the metadata of stored documents and to execute those filters.\\n\\n![](https://drive.google.com/uc?id=1OQUN-0MJcDUxmPXofgS7MqReEs720pqS)\\n\\n## Get started\\u200b\\n\\nWe\\'ll use a Pinecone vector store in this example.\\n\\nFirst we\\'ll want to create a `Pinecone` vector store and seed it with some data. We\\'ve created a small demo set of documents that contain summaries of movies.\\n\\nTo use Pinecone, you need to have `pinecone` package installed and you must have an API key and an environment. Here are the [installation instructions](https://docs.pinecone.io/docs/quickstart).\\n\\n**Note:** The self-query retriever requires you to have `lark` package installed.\\n\\n```python\\n# !pip install lark pinecone-client\\n```\\n\\n```python\\nimport os\\n\\nimport pinecone\\n\\npinecone.init(api_key=os.environ[\"PINECONE_API_KEY\"], environment=os.environ[\"PINECONE_ENV\"])\\n```\\n\\n```python\\nfrom langchain.schema import Document\\nfrom langchain.embeddings.openai import OpenAIEmbeddings\\nfrom langchain.vectorstores import Pinecone', metadata={'description': 'A self-querying retriever is one that, as the name suggests, has the ability to query itself. Specifically, given any natural language query, the retriever uses a query-constructing LLM chain to write a structured query and then applies that structured query to its underlying VectorStore. This allows the retriever to not only use the user-input query for semantic similarity comparison with the contents of stored documents but to also extract filters from the user query on the metadata of stored documents and to execute those filters.', 'language': 'en', 'source': 'https://python.langchain.com/docs/modules/data_connection/retrievers/self_query/', 'title': 'Self-querying | ü¶úÔ∏èüîó Langchain'}),\n",
       " Document(page_content=\"- [Parent Document Retriever](/docs/modules/data_connection/retrievers/parent_document_retriever): This allows you to create multiple embeddings per parent document, allowing you to look up smaller chunks but return larger context.\\n- [Self Query Retriever](/docs/modules/data_connection/retrievers/self_query): User questions often contain a reference to something that isn't just semantic but rather expresses some logic that can best be represented as a metadata filter. Self-query allows you to parse out the _semantic_ part of a query from other _metadata filters_ present in the query.\\n- [Ensemble Retriever](/docs/modules/data_connection/retrievers/ensemble): Sometimes you may want to retrieve documents from multiple different sources, or using multiple different algorithms. The ensemble retriever allows you to easily do this.\\n- And more!\", metadata={'description': \"Many LLM applications require user-specific data that is not part of the model's training set.\", 'language': 'en', 'source': 'https://python.langchain.com/docs/modules/data_connection/', 'title': 'Retrieval | ü¶úÔ∏èüîó Langchain'}),\n",
       " Document(page_content='Again, we can use the [LangSmith trace](https://smith.langchain.com/public/18460363-0c70-4c72-81c7-3b57253bb58c/r) to explore the prompt structure.\\n\\n### Going deeper\\u200b\\n\\n- Agents, such as the [conversational retrieval agent](/docs/use_cases/question_answering/how_to/conversational_retrieval_agents), can be used for retrieval when necessary while also holding a conversation.', metadata={'description': 'Open In Collab', 'language': 'en', 'source': 'https://python.langchain.com/docs/use_cases/chatbots', 'title': 'Chatbots | ü¶úÔ∏èüîó Langchain'}),\n",
       " Document(page_content='for Zep - A long-term memory store for LLM applications.](/docs/integrations/retrievers/zep_memorystore)', metadata={'language': 'en', 'source': 'https://python.langchain.com/docs/integrations/retrievers', 'title': 'Retrievers | ü¶úÔ∏èüîó Langchain'}),\n",
       " Document(page_content='- ‚õì [How to use BGE Embeddings for LangChain](https://youtu.be/sWRvSG7vL4g?si=85jnvnmTCF9YIWXI)\\n- ‚õì [How to use Custom Prompts for RetrievalQA on LLaMA-2 7B](https://youtu.be/PDwUKves9GY?si=sMF99TWU0p4eiK80)', metadata={'description': 'Below are links to tutorials and courses on LangChain. For written guides on common use cases for LangChain, check out the use cases guides.', 'language': 'en', 'source': 'https://python.langchain.com/docs/additional_resources/tutorials', 'title': 'Tutorials | ü¶úÔ∏èüîó Langchain'}),\n",
       " Document(page_content='```python\\nllm_with_tools = llm.bind(\\n    functions=[\\n        # The retriever tool\\n        format_tool_to_openai_function(retriever_tool), \\n        # Response schema\\n        convert_pydantic_to_openai_function(Response)\\n    ]\\n)\\n```\\n\\n```python\\nagent = {\\n    \"input\": lambda x: x[\"input\"],\\n    # Format agent scratchpad from intermediate steps\\n    \"agent_scratchpad\": lambda x: format_to_openai_functions(x[\\'intermediate_steps\\'])\\n} | prompt | llm_with_tools | parse\\n```\\n\\n```python\\nagent_executor = AgentExecutor(tools=[retriever_tool], agent=agent, verbose=True)\\n```\\n\\n## Run the agent\\u200b\\n\\nWe can now run the agent! Notice how it responds with a dictionary with two keys: `answer` and `sources`\\n\\n```python\\nagent_executor.invoke({\"input\": \"what did the president say about kentaji brown jackson\"}, return_only_outputs=True)\\n```', metadata={'description': 'This notebook covers how to have an agent return a structured output.', 'language': 'en', 'source': 'https://python.langchain.com/docs/modules/agents/how_to/agent_structured', 'title': 'Returning Structured Output | ü¶úÔ∏èüîó Langchain'}),\n",
       " Document(page_content='This time the answer is correct, since the self-querying retriever created a filter on the landlord attribute of the metadata, correctly filtering to document that specifically is about the DHA Group landlord. The resulting source chunks are all relevant to this landlord, and this improves answer accuracy even though the landlord is not directly mentioned in the specific chunk that contains the correct answer.', metadata={'description': 'This notebook covers how to load documents from Docugami. It provides the advantages of using this system over alternative data loaders.', 'language': 'en', 'source': 'https://python.langchain.com/docs/integrations/document_loaders/docugami', 'title': 'Docugami | ü¶úÔ∏èüîó Langchain'})]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relevant_docs = retriever.get_relevant_documents(\n",
    "    \"How to use LCEL ainvoke with a retriever?\"\n",
    ")\n",
    "relevant_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reordenado de documentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content=\"QdrantTranslator\\\\n6. WeaviateTranslator\\\\n\\\\nAnd remote retrievers like:\\\\n\\\\n1. RemoteLangChainRetriever'}\", metadata={'description': \"In this tutorial, we are going to use Langchain + Activeloop's Deep Lake with GPT to analyze the code base of the LangChain itself.\", 'language': 'en', 'source': 'https://python.langchain.com/docs/use_cases/question_answering/how_to/code/code-analysis-deeplake', 'title': \"Use LangChain, GPT and Activeloop's Deep Lake to work with code base | ü¶úÔ∏èüîó Langchain\"}),\n",
       " Document(page_content='# Self-querying\\n\\nA self-querying retriever is one that, as the name suggests, has the ability to query itself. Specifically, given any natural language query, the retriever uses a query-constructing LLM chain to write a structured query and then applies that structured query to its underlying VectorStore. This allows the retriever to not only use the user-input query for semantic similarity comparison with the contents of stored documents but to also extract filters from the user query on the metadata of stored documents and to execute those filters.\\n\\n![](https://drive.google.com/uc?id=1OQUN-0MJcDUxmPXofgS7MqReEs720pqS)\\n\\n## Get started\\u200b\\n\\nWe\\'ll use a Pinecone vector store in this example.\\n\\nFirst we\\'ll want to create a `Pinecone` vector store and seed it with some data. We\\'ve created a small demo set of documents that contain summaries of movies.\\n\\nTo use Pinecone, you need to have `pinecone` package installed and you must have an API key and an environment. Here are the [installation instructions](https://docs.pinecone.io/docs/quickstart).\\n\\n**Note:** The self-query retriever requires you to have `lark` package installed.\\n\\n```python\\n# !pip install lark pinecone-client\\n```\\n\\n```python\\nimport os\\n\\nimport pinecone\\n\\npinecone.init(api_key=os.environ[\"PINECONE_API_KEY\"], environment=os.environ[\"PINECONE_ENV\"])\\n```\\n\\n```python\\nfrom langchain.schema import Document\\nfrom langchain.embeddings.openai import OpenAIEmbeddings\\nfrom langchain.vectorstores import Pinecone', metadata={'description': 'A self-querying retriever is one that, as the name suggests, has the ability to query itself. Specifically, given any natural language query, the retriever uses a query-constructing LLM chain to write a structured query and then applies that structured query to its underlying VectorStore. This allows the retriever to not only use the user-input query for semantic similarity comparison with the contents of stored documents but to also extract filters from the user query on the metadata of stored documents and to execute those filters.', 'language': 'en', 'source': 'https://python.langchain.com/docs/modules/data_connection/retrievers/self_query/', 'title': 'Self-querying | ü¶úÔ∏èüîó Langchain'}),\n",
       " Document(page_content='Again, we can use the [LangSmith trace](https://smith.langchain.com/public/18460363-0c70-4c72-81c7-3b57253bb58c/r) to explore the prompt structure.\\n\\n### Going deeper\\u200b\\n\\n- Agents, such as the [conversational retrieval agent](/docs/use_cases/question_answering/how_to/conversational_retrieval_agents), can be used for retrieval when necessary while also holding a conversation.', metadata={'description': 'Open In Collab', 'language': 'en', 'source': 'https://python.langchain.com/docs/use_cases/chatbots', 'title': 'Chatbots | ü¶úÔ∏èüîó Langchain'}),\n",
       " Document(page_content='- ‚õì [How to use BGE Embeddings for LangChain](https://youtu.be/sWRvSG7vL4g?si=85jnvnmTCF9YIWXI)\\n- ‚õì [How to use Custom Prompts for RetrievalQA on LLaMA-2 7B](https://youtu.be/PDwUKves9GY?si=sMF99TWU0p4eiK80)', metadata={'description': 'Below are links to tutorials and courses on LangChain. For written guides on common use cases for LangChain, check out the use cases guides.', 'language': 'en', 'source': 'https://python.langchain.com/docs/additional_resources/tutorials', 'title': 'Tutorials | ü¶úÔ∏èüîó Langchain'}),\n",
       " Document(page_content='This time the answer is correct, since the self-querying retriever created a filter on the landlord attribute of the metadata, correctly filtering to document that specifically is about the DHA Group landlord. The resulting source chunks are all relevant to this landlord, and this improves answer accuracy even though the landlord is not directly mentioned in the specific chunk that contains the correct answer.', metadata={'description': 'This notebook covers how to load documents from Docugami. It provides the advantages of using this system over alternative data loaders.', 'language': 'en', 'source': 'https://python.langchain.com/docs/integrations/document_loaders/docugami', 'title': 'Docugami | ü¶úÔ∏èüîó Langchain'}),\n",
       " Document(page_content='```python\\nllm_with_tools = llm.bind(\\n    functions=[\\n        # The retriever tool\\n        format_tool_to_openai_function(retriever_tool), \\n        # Response schema\\n        convert_pydantic_to_openai_function(Response)\\n    ]\\n)\\n```\\n\\n```python\\nagent = {\\n    \"input\": lambda x: x[\"input\"],\\n    # Format agent scratchpad from intermediate steps\\n    \"agent_scratchpad\": lambda x: format_to_openai_functions(x[\\'intermediate_steps\\'])\\n} | prompt | llm_with_tools | parse\\n```\\n\\n```python\\nagent_executor = AgentExecutor(tools=[retriever_tool], agent=agent, verbose=True)\\n```\\n\\n## Run the agent\\u200b\\n\\nWe can now run the agent! Notice how it responds with a dictionary with two keys: `answer` and `sources`\\n\\n```python\\nagent_executor.invoke({\"input\": \"what did the president say about kentaji brown jackson\"}, return_only_outputs=True)\\n```', metadata={'description': 'This notebook covers how to have an agent return a structured output.', 'language': 'en', 'source': 'https://python.langchain.com/docs/modules/agents/how_to/agent_structured', 'title': 'Returning Structured Output | ü¶úÔ∏èüîó Langchain'}),\n",
       " Document(page_content='for Zep - A long-term memory store for LLM applications.](/docs/integrations/retrievers/zep_memorystore)', metadata={'language': 'en', 'source': 'https://python.langchain.com/docs/integrations/retrievers', 'title': 'Retrievers | ü¶úÔ∏èüîó Langchain'}),\n",
       " Document(page_content=\"- [Parent Document Retriever](/docs/modules/data_connection/retrievers/parent_document_retriever): This allows you to create multiple embeddings per parent document, allowing you to look up smaller chunks but return larger context.\\n- [Self Query Retriever](/docs/modules/data_connection/retrievers/self_query): User questions often contain a reference to something that isn't just semantic but rather expresses some logic that can best be represented as a metadata filter. Self-query allows you to parse out the _semantic_ part of a query from other _metadata filters_ present in the query.\\n- [Ensemble Retriever](/docs/modules/data_connection/retrievers/ensemble): Sometimes you may want to retrieve documents from multiple different sources, or using multiple different algorithms. The ensemble retriever allows you to easily do this.\\n- And more!\", metadata={'description': \"Many LLM applications require user-specific data that is not part of the model's training set.\", 'language': 'en', 'source': 'https://python.langchain.com/docs/modules/data_connection/', 'title': 'Retrieval | ü¶úÔ∏èüîó Langchain'}),\n",
       " Document(page_content='# Retrieve as you generate with FLARE\\n\\nThis notebook is an implementation of Forward-Looking Active REtrieval augmented generation (FLARE).\\n\\nPlease see the original repo [here](https://github.com/jzbjyb/FLARE/tree/main).\\n\\nThe basic idea is:\\n\\n- Start answering a question\\n- If you start generating tokens the model is uncertain about, look up relevant documents\\n- Use those documents to continue generating\\n- Repeat until finished\\n\\nThere is a lot of cool detail in how the lookup of relevant documents is done.\\nBasically, the tokens that model is uncertain about are highlighted, and then an LLM is called to generate a question that would lead to that answer. For example, if the generated text is `Joe Biden went to Harvard`, and the tokens the model was uncertain about was `Harvard`, then a good generated question would be `where did Joe Biden go to college`. This generated question is then used in a retrieval step to fetch relevant documents.\\n\\nIn order to set up this chain, we will need three things:\\n\\n- An LLM to generate the answer\\n- An LLM to generate hypothetical questions to use in retrieval\\n- A retriever to use to look up answers for\\n\\nThe LLM that we use to generate the answer needs to return logprobs so we can identify uncertain tokens. For that reason, we HIGHLY recommend that you use the OpenAI wrapper (NB: not the ChatOpenAI wrapper, as that does not return logprobs).\\n\\nThe LLM we use to generate hypothetical questions to use in retrieval can be anything. In this notebook we will use ChatOpenAI because it is fast and cheap.', metadata={'description': 'This notebook is an implementation of Forward-Looking Active REtrieval augmented generation (FLARE).', 'language': 'en', 'source': 'https://python.langchain.com/docs/use_cases/question_answering/how_to/flare', 'title': 'Retrieve as you generate with FLARE | ü¶úÔ∏èüîó Langchain'}),\n",
       " Document(page_content='## LLMRails as a Retriever\\u200b\\n\\nLLMRails, as all the other LangChain vectorstores, is most often used as a LangChain Retriever:\\n\\n```python\\nretriever = llm_rails.as_retriever()\\nretriever\\n```\\n\\n```text\\n    LLMRailsRetriever(tags=None, metadata=None, vectorstore=<langchain.vectorstores.llm_rails.LLMRails object at 0x107b9c040>, search_type=\\'similarity\\', search_kwargs={\\'k\\': 5})\\n```\\n\\n```python\\nquery = \"What is your approach to national defense\"\\nretriever.get_relevant_documents(query)[0]\\n```', metadata={'description': 'LLMRails is a API platform for building GenAI applications. It provides an easy-to-use API for document indexing and querying that is managed by LLMRails and is optimized for performance and accuracy.', 'language': 'en', 'source': 'https://python.langchain.com/docs/integrations/vectorstores/llm_rails', 'title': 'LLMRails | ü¶úÔ∏èüîó Langchain'})]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reordering = LongContextReorder()\n",
    "reordered_docs = list(reordering.transform_documents(relevant_docs))\n",
    "reordered_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uso del reordenador en nuestro pipeline de `Retrival Augmented Generation`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_documents(documents: list[Document]) -> str:\n",
    "    return \"\\n\\n\".join([doc.page_content for doc in documents])\n",
    "\n",
    "\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"\"\"Given the following text extracts:\n",
    "-----\n",
    "{context}\n",
    "-----\n",
    "                                      \n",
    "Answer the following question, if you don't know the answer, just write \"I don't know.\n",
    "\n",
    "Question: {question}\"\"\"\n",
    ")\n",
    "\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "\n",
    "stuff_chain = (\n",
    "    {\n",
    "        \"context\": itemgetter(\"question\")\n",
    "        | retriever\n",
    "        | reordering.transform_documents\n",
    "        | combine_documents,\n",
    "        \"question\": itemgetter(\"question\"),\n",
    "    }\n",
    "    | prompt\n",
    "    | llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To create a chain using LCEL, you can follow these steps:\n",
      "\n",
      "1. Import the necessary modules and classes from the LangChain library.\n",
      "2. Define the components of your chain, such as LLMs, prompts, and tools.\n",
      "3. Use the LCEL syntax to chain together the components in the desired order.\n",
      "4. Invoke the chain with the input data to get the output.\n",
      "\n",
      "Here is an example of creating a chain using LCEL:\n",
      "\n",
      "```python\n",
      "from langchain.prompts.prompt import PromptTemplate\n",
      "from langchain.chat_models import ChatOpenAI\n",
      "from langchain.anonymizers import PresidioReversibleAnonymizer\n",
      "\n",
      "# Define the components\n",
      "anonymizer = PresidioReversibleAnonymizer()\n",
      "prompt = PromptTemplate.from_template(template=\"{anonymized_text}\")\n",
      "llm = ChatOpenAI(temperature=0)\n",
      "\n",
      "# Chain the components together\n",
      "chain = {\"anonymized_text\": anonymizer.anonymize} | prompt | llm\n",
      "\n",
      "# Invoke the chain with input data\n",
      "text = \"This is a sample text.\"\n",
      "response = chain.invoke(text)\n",
      "\n",
      "# Get the output\n",
      "output = response.content\n",
      "print(output)\n",
      "```\n",
      "\n",
      "In this example, the chain starts with the `anonymizer.anonymize` function, which anonymizes the input text. The anonymized text is then passed to the `prompt` component, which generates a prompt using the anonymized text. Finally, the prompt is passed to the `llm` component, which generates the output response.\n",
      "\n",
      "Note that this is just a basic example, and you can customize the chain by adding more components or modifying the existing ones according to your requirements.\n"
     ]
    }
   ],
   "source": [
    "response = stuff_chain.invoke(input={\"question\": \"How to create a chain using LCEL?\"}).content\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
