{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self Retrievers\n",
    "\n",
    "![Self Retrievers](https://drive.google.com/uc?id=1OQUN-0MJcDUxmPXofgS7MqReEs720pqS)\n",
    "\n",
    "Un recuperador autoconsultante puede analizar y entender las consultas que se le hacen en lenguaje natural, y luego, puede buscar y filtrar informaci√≥n relevante de su base de datos o documentos almacenados bas√°ndose en esas consultas. Esto lo hace transformando las consultas en un formato estructurado que puede interpretar y procesar de manera eficiente. Esto significa que, adem√°s de comparar la consulta del usuario con los documentos para encontrar coincidencias, tambi√©n puede filtrar los resultados seg√∫n criterios espec√≠ficos extra√≠dos de la consulta del usuario."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Librer√≠as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from langchain.chains import create_tagging_chain_pydantic\n",
    "from langchain.chains.query_constructor.base import AttributeInfo\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.indexes import SQLRecordManager, index\n",
    "from langchain.retrievers import SelfQueryRetriever\n",
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import Language, RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from src.langchain_docs_loader import LangchainDocsLoader, num_tokens_from_string\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carga de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3190"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "    language=Language.MARKDOWN,\n",
    "    chunk_size=400,\n",
    "    chunk_overlap=50,\n",
    "    length_function=num_tokens_from_string,\n",
    ")\n",
    "\n",
    "loader = LangchainDocsLoader(include_output_cells=False)\n",
    "docs = loader.load()\n",
    "docs = text_splitter.split_documents(docs)\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [doc for doc in docs if doc.page_content != \"```\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inicializado de modelo de lenguaje"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etiquetado de documentos\n",
    "\n",
    "Los documentos por s√≠ mismos son √∫tiles, pero cuando son etiquetados con informaci√≥n adicional, pueden volverse m√°s √∫tiles. Por ejemplo, si etiquetamos los documentos con su idioma, podemos filtrar los documentos que no est√©n en el idioma que nos interesa. Si etiquetamos los documentos con su tema, podemos filtrar los documentos que no est√©n relacionados con el tema que nos interesa. De esta manera, podemos reducir el espacio de b√∫squeda y obtener mejores resultados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creaci√≥n de esquema de etiquetas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'properties': {'code_snippet': {'default': False,\n",
      "                                 'description': 'Whether the text fragment '\n",
      "                                                'includes a code snippet. Code '\n",
      "                                                'snippets are valid markdown '\n",
      "                                                'code blocks.',\n",
      "                                 'title': 'Code Snippet',\n",
      "                                 'type': 'boolean'},\n",
      "                'completness': {'description': 'Describes how useful is the '\n",
      "                                               'text in terms of '\n",
      "                                               'self-explanation. It is '\n",
      "                                               'critical to excel.',\n",
      "                                'enum': ['Very',\n",
      "                                         'Quite',\n",
      "                                         'Medium',\n",
      "                                         'Little',\n",
      "                                         'Not'],\n",
      "                                'title': 'Completness',\n",
      "                                'type': 'string'},\n",
      "                'contains_markdown_table': {'default': False,\n",
      "                                            'description': 'Whether the text '\n",
      "                                                           'fragment contains '\n",
      "                                                           'a markdown table.',\n",
      "                                            'title': 'Contains Markdown Table',\n",
      "                                            'type': 'boolean'},\n",
      "                'description': {'default': False,\n",
      "                                'description': 'Whether the text fragment '\n",
      "                                               'includes a description.',\n",
      "                                'title': 'Description',\n",
      "                                'type': 'boolean'},\n",
      "                'talks_about_chain': {'default': False,\n",
      "                                      'description': 'Whether the text '\n",
      "                                                     'fragment talks about a '\n",
      "                                                     'chain.',\n",
      "                                      'title': 'Talks About Chain',\n",
      "                                      'type': 'boolean'},\n",
      "                'talks_about_expression_language': {'default': False,\n",
      "                                                    'description': 'Whether '\n",
      "                                                                   'the text '\n",
      "                                                                   'fragment '\n",
      "                                                                   'talks '\n",
      "                                                                   'about an '\n",
      "                                                                   'langchain '\n",
      "                                                                   'expression '\n",
      "                                                                   'language.',\n",
      "                                                    'title': 'Talks About '\n",
      "                                                             'Expression '\n",
      "                                                             'Language',\n",
      "                                                    'type': 'boolean'},\n",
      "                'talks_about_retriever': {'default': False,\n",
      "                                          'description': 'Whether the text '\n",
      "                                                         'fragment talks about '\n",
      "                                                         'a retriever.',\n",
      "                                          'title': 'Talks About Retriever',\n",
      "                                          'type': 'boolean'},\n",
      "                'talks_about_vectorstore': {'default': False,\n",
      "                                            'description': 'Whether the text '\n",
      "                                                           'fragment talks '\n",
      "                                                           'about a '\n",
      "                                                           'vectorstore.',\n",
      "                                            'title': 'Talks About Vectorstore',\n",
      "                                            'type': 'boolean'}},\n",
      " 'required': ['completness'],\n",
      " 'title': 'Tags',\n",
      " 'type': 'object'}\n"
     ]
    }
   ],
   "source": [
    "class Tags(BaseModel):\n",
    "    completness: str = Field(\n",
    "        description=\"Describes how useful is the text in terms of self-explanation. It is critical to excel.\",\n",
    "        enum=[\"Very\", \"Quite\", \"Medium\", \"Little\", \"Not\"],\n",
    "    )\n",
    "    code_snippet: bool = Field(\n",
    "        default=False,\n",
    "        description=\"Whether the text fragment includes a code snippet. Code snippets are valid markdown code blocks.\",\n",
    "    )\n",
    "    description: bool = Field(\n",
    "        default=False, description=\"Whether the text fragment includes a description.\"\n",
    "    )\n",
    "    talks_about_vectorstore: bool = Field(\n",
    "        default=False,\n",
    "        description=\"Whether the text fragment talks about a vectorstore.\",\n",
    "    )\n",
    "    talks_about_retriever: bool = Field(\n",
    "        default=False, description=\"Whether the text fragment talks about a retriever.\"\n",
    "    )\n",
    "    talks_about_chain: bool = Field(\n",
    "        default=False, description=\"Whether the text fragment talks about a chain.\"\n",
    "    )\n",
    "    talks_about_expression_language: bool = Field(\n",
    "        default=False,\n",
    "        description=\"Whether the text fragment talks about an langchain expression language.\",\n",
    "    )\n",
    "    contains_markdown_table: bool = Field(\n",
    "        default=False,\n",
    "        description=\"Whether the text fragment contains a markdown table.\",\n",
    "    )\n",
    "\n",
    "\n",
    "pprint(Tags.schema())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creaci√≥n de cadena de generaci√≥n de etiquetas (etiquetador)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagging_prompt = \"\"\"Extract the desired information from the following passage.\n",
    "\n",
    "Only extract the properties mentioned in the 'information_extraction' function.\n",
    "Completness should involve more than one sentence.\n",
    "To consider that a passage talks about a property, it is enough that it mentions it once.\n",
    "If there is no mention of a property, set it to False. It only applies for the talk_about_* properties.\n",
    "\n",
    "For instance,\n",
    "To set `talks_about_vectorstore` to True, document should contain the word 'vectorstore' at least once.\n",
    "To set `talks_about_retriever` to True, document should contain the word 'retriever' at least once.\n",
    "To set `talks_about_chain` to True, document should contain the word 'chain' at least once.\n",
    "To set `talks_about_expression_language` to True, document should contain the word 'expression language' or 'LCEL' at least once.\n",
    "\n",
    "Passage:\n",
    "{input}\n",
    "\"\"\"\n",
    "\n",
    "tagging_chain = create_tagging_chain_pydantic(Tags, llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplos de uso del etiquetador"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probablemente, un fragmento que √∫nicamente contiene una lista de enlaces a otros fragmentos que tambi√©n se encuentran indexados no es muy √∫til. Esto podr√≠a ocasionar que recuperemos un documento que no es relevante para la consulta, mientras el documento que s√≠ es relevante no se encuentre en los primeros lugares de la lista de resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[üìÑÔ∏è DependentsDependents stats for langchain-ai/langchain](/docs/additional_resources/dependents)[üìÑÔ∏è TutorialsBelow are links to tutorials and courses on LangChain. For written guides on common use cases for LangChain, check out the use cases guides.](/docs/additional_resources/tutorials)[üìÑÔ∏è YouTube videos‚õì icon marks a new addition [last update 2023-09-21]](/docs/additional_resources/youtube)[üîó Gallery](https://github.com/kyrolabs/awesome-langchain)\n",
      "{'code_snippet': False,\n",
      " 'completness': 'Not',\n",
      " 'contains_markdown_table': False,\n",
      " 'description': True,\n",
      " 'talks_about_chain': True,\n",
      " 'talks_about_expression_language': True,\n",
      " 'talks_about_retriever': True,\n",
      " 'talks_about_vectorstore': True}\n"
     ]
    }
   ],
   "source": [
    "idx = 0\n",
    "\n",
    "result = tagging_chain.invoke(input={\"input\": docs[idx].page_content})\n",
    "print(result.get(\"input\"))\n",
    "pprint(result.get(\"text\").dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un fragmento con enlace a su documentaci√≥n y ejemplo de uso ser√≠a m√°s √∫til."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# AWS DynamoDB\n",
      "\n",
      "[Amazon AWS DynamoDB](https://awscli.amazonaws.com/v2/documentation/api/latest/reference/dynamodb/index.html) is a fully managed `NoSQL` database service that provides fast and predictable performance with seamless scalability.\n",
      "\n",
      "This notebook goes over how to use `DynamoDB` to store chat message history.\n",
      "\n",
      "First make sure you have correctly configured the [AWS CLI](https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html). Then make sure you have installed `boto3`.\n",
      "\n",
      "```bash\n",
      "pip install boto3\n",
      "```\n",
      "\n",
      "Next, create the `DynamoDB` Table where we will be storing messages:\n",
      "\n",
      "```python\n",
      "import boto3\n",
      "\n",
      "# Get the service resource.\n",
      "dynamodb = boto3.resource(\"dynamodb\")\n",
      "\n",
      "# Create the DynamoDB table.\n",
      "table = dynamodb.create_table(\n",
      "    TableName=\"SessionTable\",\n",
      "    KeySchema=[{\"AttributeName\": \"SessionId\", \"KeyType\": \"HASH\"}],\n",
      "    AttributeDefinitions=[{\"AttributeName\": \"SessionId\", \"AttributeType\": \"S\"}],\n",
      "    BillingMode=\"PAY_PER_REQUEST\",\n",
      ")\n",
      "\n",
      "# Wait until the table exists.\n",
      "table.meta.client.get_waiter(\"table_exists\").wait(TableName=\"SessionTable\")\n",
      "\n",
      "# Print out some data about the table.\n",
      "print(table.item_count)\n",
      "```\n",
      "\n",
      "## DynamoDBChatMessageHistory‚Äã\n",
      "\n",
      "```python\n",
      "from langchain.memory.chat_message_histories import DynamoDBChatMessageHistory\n",
      "\n",
      "history = DynamoDBChatMessageHistory(table_name=\"SessionTable\", session_id=\"0\")\n",
      "\n",
      "history.add_user_message(\"hi!\")\n",
      "\n",
      "history.add_ai_message(\"whats up?\")\n",
      "```\n",
      "\n",
      "> **API Reference:**\n",
      "> - [DynamoDBChatMessageHistory](https://api.python.langchain.com/en/latest/memory/langchain.memory.chat_message_histories.dynamodb.DynamoDBChatMessageHistory.html)\n",
      "\n",
      "```python\n",
      "history.messages\n",
      "```\n",
      "{'code_snippet': True,\n",
      " 'completness': 'Very',\n",
      " 'contains_markdown_table': True,\n",
      " 'description': True,\n",
      " 'talks_about_chain': True,\n",
      " 'talks_about_expression_language': True,\n",
      " 'talks_about_retriever': True,\n",
      " 'talks_about_vectorstore': True}\n"
     ]
    }
   ],
   "source": [
    "idx = 1000\n",
    "\n",
    "result = tagging_chain.invoke(input={\"input\": docs[idx].page_content})\n",
    "print(result.get(\"input\"))\n",
    "pprint(result.get(\"text\").dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text content from PubMed Central and publisher web sites.](/docs/integrations/retrievers/pubmed)[üìÑÔ∏è RePhraseQueryRetrieverSimple retriever that applies an LLM between the user input and the query pass the to retriever.](/docs/integrations/retrievers/re_phrase)[üìÑÔ∏è SEC filings dataSEC filings data powered by Kay.ai and Cybersyn.](/docs/integrations/retrievers/sec_filings)[üìÑÔ∏è SVMSupport vector machines (SVMs) are a set of supervised learning methods used for classification, regression and outliers detection.](/docs/integrations/retrievers/svm)[üìÑÔ∏è TF-IDFTF-IDF means term-frequency times inverse document-frequency.](/docs/integrations/retrievers/tf_idf)[üìÑÔ∏è VespaVespa is a fully featured search engine and vector database. It supports vector search (ANN), lexical search, and search in structured data, all in the same query.](/docs/integrations/retrievers/vespa)[üìÑÔ∏è Weaviate Hybrid SearchWeaviate is an open source vector database.](/docs/integrations/retrievers/weaviate-hybrid)[üìÑÔ∏è WikipediaWikipedia is a multilingual free online encyclopedia written and maintained by a community of volunteers, known as Wikipedians, through open collaboration and using a wiki-based editing system called MediaWiki. Wikipedia is the largest and most-read reference work in history.](/docs/integrations/retrievers/wikipedia)[üìÑÔ∏è ZepRetriever Example for Zep - A long-term memory store for LLM applications.](/docs/integrations/retrievers/zep_memorystore)\n",
      "{'code_snippet': False,\n",
      " 'completness': 'Not',\n",
      " 'contains_markdown_table': False,\n",
      " 'description': False,\n",
      " 'talks_about_chain': False,\n",
      " 'talks_about_expression_language': False,\n",
      " 'talks_about_retriever': True,\n",
      " 'talks_about_vectorstore': False}\n"
     ]
    }
   ],
   "source": [
    "idx = 1400\n",
    "\n",
    "result = tagging_chain.invoke(input={\"input\": docs[idx].page_content})\n",
    "print(result.get(\"input\"))\n",
    "pprint(result.get(\"text\").dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Etiquetado de documentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Documents with tags: 184'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagging_results = tagging_chain.batch(\n",
    "    inputs=[{\"input\": doc.page_content} for doc in docs[:200]],\n",
    "    return_exceptions=True,\n",
    "    config={\n",
    "        \"max_concurrency\": 50,\n",
    "    },\n",
    ")\n",
    "\n",
    "docs_with_tags = [\n",
    "    Document(\n",
    "        page_content=doc.page_content,\n",
    "        metadata={\n",
    "            **doc.metadata,\n",
    "            **result.get(\"text\").dict(),\n",
    "        },\n",
    "    )\n",
    "    for doc, result in zip(docs, tagging_results)\n",
    "    if not isinstance(result, Exception)\n",
    "]\n",
    "\n",
    "f\"Documents with tags: {len(docs_with_tags)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexado de documentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num_added': 184, 'num_updated': 0, 'num_skipped': 0, 'num_deleted': 0}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorstore = Chroma(\n",
    "    collection_name=\"langchain_docs\",\n",
    "    embedding_function=OpenAIEmbeddings(),\n",
    ")\n",
    "\n",
    "record_manager = SQLRecordManager(\n",
    "    db_url=\"sqlite:///:memory:\",\n",
    "    namespace=\"chroma/langchain_docs\",\n",
    ")\n",
    "\n",
    "record_manager.create_schema()\n",
    "\n",
    "index(\n",
    "    docs_source=docs_with_tags,\n",
    "    record_manager=record_manager,\n",
    "    vector_store=vectorstore,\n",
    "    cleanup=\"full\",\n",
    "    source_id_key=\"source\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recuperaci√≥n de documentos con un `Self Retriever`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creaci√≥n de interfaz de los metadatos disponibles en el √≠ndice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_field_info = [\n",
    "    AttributeInfo(\n",
    "        name=\"completness\",\n",
    "        description=\"Describes how useful is the text in terms of self-explanation. It is critical to excel.\",\n",
    "        type='enum=[\"Very\", \"Quite\", \"Medium\", \"Little\", \"Not\"]',\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"code_snippet\",\n",
    "        description=\"Whether the text fragment includes a code snippet. Code snippets are valid markdown code blocks.\",\n",
    "        type=\"bool\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"description\",\n",
    "        description=\"Whether the text fragment includes a description.\",\n",
    "        type=\"bool\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"talks_about_vectorstore\",\n",
    "        description=\"Whether the text fragment talks about a vectorstore.\",\n",
    "        type=\"bool\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"talks_about_retriever\",\n",
    "        description=\"Whether the text fragment talks about a retriever.\",\n",
    "        type=\"bool\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"talks_about_chain\",\n",
    "        description=\"Whether the text fragment talks about a chain.\",\n",
    "        type=\"bool\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"contains_markdown_table\",\n",
    "        description=\"Whether the text fragment contains a markdown table.\",\n",
    "        type=\"bool\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "document_content_description = \"Langchain documentation\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creaci√≥n de `retriever`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0)\n",
    "retriever = SelfQueryRetriever.from_llm(\n",
    "    llm=llm,\n",
    "    vectorstore=vectorstore,\n",
    "    document_contents=document_content_description,\n",
    "    metadata_field_info=metadata_field_info,\n",
    "    enable_limit=True,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recuperaci√≥n de documentos con el `retriever`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jvelezmagic/Documents/github/course/langchain-course/course_document_management/.venv/lib/python3.11/site-packages/langchain/chains/llm.py:280: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query='expression language retrievers' filter=Operation(operator=<Operator.AND: 'and'>, arguments=[Comparison(comparator=<Comparator.EQ: 'eq'>, attribute='completness', value='Very'), Comparison(comparator=<Comparator.EQ: 'eq'>, attribute='talks_about_retriever', value=True)]) limit=None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(page_content='```\\n\\n```python\\nchain = (\\n    {\"context\": retriever, \"question\": RunnablePassthrough()} \\n    | prompt \\n    | model \\n    | StrOutputParser()\\n)\\n```\\n\\n```python\\nchain.invoke(\"where did harrison work?\")\\n```\\n\\n```python\\ntemplate = \"\"\"Answer the question based only on the following context:\\n{context}\\n\\nQuestion: {question}\\n\\nAnswer in the following language: {language}\\n\"\"\"\\nprompt = ChatPromptTemplate.from_template(template)\\n\\nchain = {\\n    \"context\": itemgetter(\"question\") | retriever, \\n    \"question\": itemgetter(\"question\"), \\n    \"language\": itemgetter(\"language\")\\n} | prompt | model | StrOutputParser()\\n```\\n\\n```python\\nchain.invoke({\"question\": \"where did harrison work\", \"language\": \"italian\"})\\n```', metadata={'code_snippet': True, 'completness': 'Very', 'contains_markdown_table': False, 'description': True, 'language': 'en', 'source': 'https://python.langchain.com/docs/expression_language/cookbook/retrieval', 'talks_about_chain': True, 'talks_about_expression_language': True, 'talks_about_retriever': True, 'talks_about_vectorstore': False, 'title': 'RAG | ü¶úÔ∏èüîó Langchain'}),\n",
       " Document(page_content='# RAG\\n\\nLet\\'s look at adding in a retrieval step to a prompt and LLM, which adds up to a \"retrieval-augmented generation\" chain\\n\\n```bash\\npip install langchain openai faiss-cpu tiktoken\\n```\\n\\n```python\\nfrom operator import itemgetter\\n\\nfrom langchain.prompts import ChatPromptTemplate\\nfrom langchain.chat_models import ChatOpenAI\\nfrom langchain.embeddings import OpenAIEmbeddings\\nfrom langchain.schema.output_parser import StrOutputParser\\nfrom langchain.schema.runnable import RunnablePassthrough\\nfrom langchain.vectorstores import FAISS\\n```\\n\\n> **API Reference:**\\n> - [ChatPromptTemplate](https://api.python.langchain.com/en/latest/prompts/langchain.prompts.chat.ChatPromptTemplate.html)\\n> - [ChatOpenAI](https://api.python.langchain.com/en/latest/chat_models/langchain.chat_models.openai.ChatOpenAI.html)\\n> - [OpenAIEmbeddings](https://api.python.langchain.com/en/latest/embeddings/langchain.embeddings.openai.OpenAIEmbeddings.html)\\n> - [StrOutputParser](https://api.python.langchain.com/en/latest/schema/langchain.schema.output_parser.StrOutputParser.html)\\n> - [RunnablePassthrough](https://api.python.langchain.com/en/latest/schema/langchain.schema.runnable.passthrough.RunnablePassthrough.html)\\n> - [FAISS](https://api.python.langchain.com/en/latest/vectorstores/langchain.vectorstores.faiss.FAISS.html)\\n\\n```python\\nvectorstore = FAISS.from_texts([\"harrison worked at kensho\"], embedding=OpenAIEmbeddings())\\nretriever = vectorstore.as_retriever()\\n\\ntemplate = \"\"\"Answer the question based only on the following context:\\n{context}\\n\\nQuestion: {question}\\n\"\"\"\\nprompt = ChatPromptTemplate.from_template(template)\\n\\nmodel = ChatOpenAI()', metadata={'code_snippet': False, 'completness': 'Very', 'contains_markdown_table': False, 'description': False, 'language': 'en', 'source': 'https://python.langchain.com/docs/expression_language/cookbook/retrieval', 'talks_about_chain': True, 'talks_about_expression_language': True, 'talks_about_retriever': True, 'talks_about_vectorstore': False, 'title': 'RAG | ü¶úÔ∏èüîó Langchain'}),\n",
       " Document(page_content='## Manipulating outputs/inputs\\u200b\\n\\nMaps can be useful for manipulating the output of one Runnable to match the input format of the next Runnable in a sequence.\\n\\n```python\\nfrom langchain.embeddings import OpenAIEmbeddings\\nfrom langchain.schema.output_parser import StrOutputParser\\nfrom langchain.schema.runnable import RunnablePassthrough\\nfrom langchain.vectorstores import FAISS\\n\\nvectorstore = FAISS.from_texts([\"harrison worked at kensho\"], embedding=OpenAIEmbeddings())\\nretriever = vectorstore.as_retriever()\\ntemplate = \"\"\"Answer the question based only on the following context:\\n{context}\\n\\nQuestion: {question}\\n\"\"\"\\nprompt = ChatPromptTemplate.from_template(template)\\n\\nretrieval_chain = (\\n    {\"context\": retriever, \"question\": RunnablePassthrough()} \\n    | prompt \\n    | model \\n    | StrOutputParser()\\n)\\n\\nretrieval_chain.invoke(\"where did harrison work?\")', metadata={'code_snippet': True, 'completness': 'Very', 'contains_markdown_table': False, 'description': True, 'language': 'en', 'source': 'https://python.langchain.com/docs/expression_language/how_to/map', 'talks_about_chain': True, 'talks_about_expression_language': True, 'talks_about_retriever': True, 'talks_about_vectorstore': True, 'title': 'Use RunnableMaps | ü¶úÔ∏èüîó Langchain'}),\n",
       " Document(page_content='## Without References\\u200b\\n\\nWhen references aren\\'t available, you can still predict the preferred response.\\nThe results will reflect the evaluation model\\'s preference, which is less reliable and may result\\nin preferences that are factually incorrect.\\n\\n```python\\nfrom langchain.evaluation import load_evaluator\\n\\nevaluator = load_evaluator(\"pairwise_string\")\\n```\\n\\n> **API Reference:**\\n> - [load_evaluator](https://api.python.langchain.com/en/latest/evaluation/langchain.evaluation.loading.load_evaluator.html)\\n\\n```python\\nevaluator.evaluate_string_pairs(\\n    prediction=\"Addition is a mathematical operation.\",\\n    prediction_b=\"Addition is a mathematical operation that adds two numbers to create a third number, the \\'sum\\'.\",\\n    input=\"What is addition?\",\\n)\\n```', metadata={'code_snippet': True, 'completness': 'Very', 'contains_markdown_table': False, 'description': True, 'language': 'en', 'source': 'https://python.langchain.com/docs/guides/evaluation/comparison/pairwise_string', 'talks_about_chain': True, 'talks_about_expression_language': True, 'talks_about_retriever': True, 'talks_about_vectorstore': False, 'title': 'Pairwise String Comparison | ü¶úÔ∏èüîó Langchain'})]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relevant_documents = retriever.get_relevant_documents(\n",
    "    \"useful documents that talk about expression language and retrievers\"\n",
    ")\n",
    "relevant_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query='expression language' filter=Operation(operator=<Operator.AND: 'and'>, arguments=[Comparison(comparator=<Comparator.EQ: 'eq'>, attribute='completness', value='Very'), Operation(operator=<Operator.OR: 'or'>, arguments=[Comparison(comparator=<Comparator.EQ: 'eq'>, attribute='talks_about_retriever', value=True), Comparison(comparator=<Comparator.EQ: 'eq'>, attribute='talks_about_vectorstore', value=True)])]) limit=None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(page_content='# Code writing\\n\\nExample of how to use LCEL to write Python code.\\n\\n```python\\nfrom langchain.chat_models import ChatOpenAI\\nfrom langchain.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\\nfrom langchain.schema.output_parser import StrOutputParser\\nfrom langchain.utilities import PythonREPL\\n```\\n\\n> **API Reference:**\\n> - [ChatOpenAI](https://api.python.langchain.com/en/latest/chat_models/langchain.chat_models.openai.ChatOpenAI.html)\\n> - [ChatPromptTemplate](https://api.python.langchain.com/en/latest/prompts/langchain.prompts.chat.ChatPromptTemplate.html)\\n> - [SystemMessagePromptTemplate](https://api.python.langchain.com/en/latest/prompts/langchain.prompts.chat.SystemMessagePromptTemplate.html)\\n> - [HumanMessagePromptTemplate](https://api.python.langchain.com/en/latest/prompts/langchain.prompts.chat.HumanMessagePromptTemplate.html)\\n> - [StrOutputParser](https://api.python.langchain.com/en/latest/schema/langchain.schema.output_parser.StrOutputParser.html)\\n> - [PythonREPL](https://api.python.langchain.com/en/latest/utilities/langchain.utilities.python.PythonREPL.html)\\n\\n```python\\ntemplate = \"\"\"Write some python code to solve the user\\'s problem. \\n\\nReturn only python code in Markdown format, e.g.:\\n\\n```python\\n....\\n```\"\"\"\\nprompt = ChatPromptTemplate.from_messages(\\n    [(\"system\", template), (\"human\", \"{input}\")]\\n)\\n\\nmodel = ChatOpenAI()\\n```\\n\\n```python\\ndef _sanitize_output(text: str):\\n    _, after = text.split(\"```python\")\\n    return after.split(\"```\")[0]\\n```\\n\\n```python\\nchain = prompt | model | StrOutputParser() | _sanitize_output | PythonREPL().run\\n```\\n\\n```python\\nchain.invoke({\"input\": \"whats 2 plus 2\"})\\n```', metadata={'code_snippet': True, 'completness': 'Very', 'contains_markdown_table': True, 'description': True, 'language': 'en', 'source': 'https://python.langchain.com/docs/expression_language/cookbook/code_writing', 'talks_about_chain': True, 'talks_about_expression_language': True, 'talks_about_retriever': True, 'talks_about_vectorstore': True, 'title': 'Code writing | ü¶úÔ∏èüîó Langchain'}),\n",
       " Document(page_content='```\\n\\n```python\\nchain = (\\n    {\"context\": retriever, \"question\": RunnablePassthrough()} \\n    | prompt \\n    | model \\n    | StrOutputParser()\\n)\\n```\\n\\n```python\\nchain.invoke(\"where did harrison work?\")\\n```\\n\\n```python\\ntemplate = \"\"\"Answer the question based only on the following context:\\n{context}\\n\\nQuestion: {question}\\n\\nAnswer in the following language: {language}\\n\"\"\"\\nprompt = ChatPromptTemplate.from_template(template)\\n\\nchain = {\\n    \"context\": itemgetter(\"question\") | retriever, \\n    \"question\": itemgetter(\"question\"), \\n    \"language\": itemgetter(\"language\")\\n} | prompt | model | StrOutputParser()\\n```\\n\\n```python\\nchain.invoke({\"question\": \"where did harrison work\", \"language\": \"italian\"})\\n```', metadata={'code_snippet': True, 'completness': 'Very', 'contains_markdown_table': False, 'description': True, 'language': 'en', 'source': 'https://python.langchain.com/docs/expression_language/cookbook/retrieval', 'talks_about_chain': True, 'talks_about_expression_language': True, 'talks_about_retriever': True, 'talks_about_vectorstore': False, 'title': 'RAG | ü¶úÔ∏èüîó Langchain'}),\n",
       " Document(page_content='## Using Constitutional Principles\\u200b\\n\\nCustom rubrics are similar to principles from [Constitutional AI](https://arxiv.org/abs/2212.08073). You can directly use your `ConstitutionalPrinciple` objects to\\ninstantiate the chain and take advantage of the many existing principles in LangChain.\\n\\n```python\\nfrom langchain.chains.constitutional_ai.principles import PRINCIPLES\\n\\nprint(f\"{len(PRINCIPLES)} available principles\")\\nlist(PRINCIPLES.items())[:5]\\n```\\n\\n```python\\nevaluator = load_evaluator(\\n    EvaluatorType.CRITERIA, criteria=PRINCIPLES[\"harmful1\"]\\n)\\neval_result = evaluator.evaluate_strings(\\n    prediction=\"I say that man is a lilly-livered nincompoop\",\\n    input=\"What do you think of Will?\",\\n)\\nprint(eval_result)\\n```\\n\\n## Configuring the LLM\\u200b\\n\\nIf you don\\'t specify an eval LLM, the `load_evaluator` method will initialize a `gpt-4` LLM to power the grading chain. Below, use an anthropic model instead.\\n\\n```python\\n# %pip install ChatAnthropic\\n# %env ANTHROPIC_API_KEY=<API_KEY>\\n```\\n\\n```python\\nfrom langchain.chat_models import ChatAnthropic\\n\\nllm = ChatAnthropic(temperature=0)\\nevaluator = load_evaluator(\"criteria\", llm=llm, criteria=\"conciseness\")\\n```\\n\\n> **API Reference:**\\n> - [ChatAnthropic](https://api.python.langchain.com/en/latest/chat_models/langchain.chat_models.anthropic.ChatAnthropic.html)\\n\\n```python\\neval_result = evaluator.evaluate_strings(\\n    prediction=\"What\\'s 2+2? That\\'s an elementary question. The answer you\\'re looking for is that two and two is four.\",\\n    input=\"What\\'s 2+2?\",\\n)\\nprint(eval_result)\\n```', metadata={'code_snippet': True, 'completness': 'Very', 'contains_markdown_table': True, 'description': True, 'language': 'en', 'source': 'https://python.langchain.com/docs/guides/evaluation/string/criteria_eval_chain', 'talks_about_chain': True, 'talks_about_expression_language': True, 'talks_about_retriever': True, 'talks_about_vectorstore': True, 'title': 'Criteria Evaluation | ü¶úÔ∏èüîó Langchain'}),\n",
       " Document(page_content='# You can load by enum or by raw python string\\nevaluator = load_evaluator(\\n    \"embedding_distance\", distance_metric=EmbeddingDistance.EUCLIDEAN\\n)\\n```\\n\\n## Select Embeddings to Use\\u200b\\n\\nThe constructor uses `OpenAI` embeddings by default, but you can configure this however you want. Below, use huggingface local embeddings\\n\\n```python\\nfrom langchain.embeddings import HuggingFaceEmbeddings\\n\\nembedding_model = HuggingFaceEmbeddings()\\nhf_evaluator = load_evaluator(\"embedding_distance\", embeddings=embedding_model)\\n```\\n\\n> **API Reference:**\\n> - [HuggingFaceEmbeddings](https://api.python.langchain.com/en/latest/embeddings/langchain.embeddings.huggingface.HuggingFaceEmbeddings.html)\\n\\n```python\\nhf_evaluator.evaluate_strings(prediction=\"I shall go\", reference=\"I shan\\'t go\")\\n```\\n\\n```python\\nhf_evaluator.evaluate_strings(prediction=\"I shall go\", reference=\"I will go\")\\n```\\n\\n[](None)_1. Note: When it comes to semantic similarity, this often gives better results than older string distance metrics (such as those in the [StringDistanceEvalChain](https://api.python.langchain.com/en/latest/evaluation/langchain.evaluation.string_distance.base.StringDistanceEvalChain.html#langchain.evaluation.string_distance.base.StringDistanceEvalChain)), though it tends to be less reliable than evaluators that use the LLM directly (such as the [QAEvalChain](https://api.python.langchain.com/en/latest/evaluation/langchain.evaluation.qa.eval_chain.QAEvalChain.html#langchain.evaluation.qa.eval_chain.QAEvalChain) or [LabeledCriteriaEvalChain](https://api.python.langchain.com/en/latest/evaluation/langchain.evaluation.criteria.eval_chain.LabeledCriteriaEvalChain.html#langchain.evaluation.criteria.eval_chain.LabeledCriteriaEvalChain)) _', metadata={'code_snippet': True, 'completness': 'Very', 'contains_markdown_table': True, 'description': True, 'language': 'en', 'source': 'https://python.langchain.com/docs/guides/evaluation/string/embedding_distance', 'talks_about_chain': True, 'talks_about_expression_language': True, 'talks_about_retriever': True, 'talks_about_vectorstore': True, 'title': 'Embedding Distance | ü¶úÔ∏èüîó Langchain'})]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relevant_documents = retriever.get_relevant_documents(\n",
    "    \"useful documents that talk about expression language and retrievers or vectorstores\"\n",
    ")\n",
    "relevant_documents"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
